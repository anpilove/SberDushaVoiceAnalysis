{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0IvyV6_EdcFa",
   "metadata": {
    "id": "0IvyV6_EdcFa"
   },
   "source": [
    "## Cодержание:\n",
    "* [Импорт библиотек](#first)\n",
    "* [Загрузка и изучение данных](#second)\n",
    "* [Базовый анализ данных](#third)\n",
    "* [Предобработка данных](#fourth)\n",
    "* [Обучение модели](#fifth)\n",
    "* [Тестирование модели](#sixth)\n",
    "* [Выводы](#seventh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m4Hw1xcOeRlB",
   "metadata": {
    "id": "m4Hw1xcOeRlB"
   },
   "source": [
    "## Импорт библиотек <a class=\"anchor\" id=\"first\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8880ce28-f137-465a-aad3-0dcd008a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install noisereduce\n",
    "# pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "OMe8JqWHeavp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "OMe8JqWHeavp",
    "outputId": "05181c2e-7d19-4bea-9335-521739e84024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from IPython.display import Audio\n",
    "import noisereduce as nr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfba834-685d-4547-bbc1-3afd72cd7b53",
   "metadata": {},
   "source": [
    "## Функция обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0451fe-51d4-4ace-b844-5de00d4915c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, n_epochs, train_loader, test_loader):\n",
    "\n",
    "  loss_train = []\n",
    "  accuracy_train = []\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for input, target in tqdm(train_loader, desc=f\"Training epoch {epoch + 1}/{n_epochs}\"):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for input, target in tqdm(test_loader, desc=f\"Testing epoch {epoch + 1}/{n_epochs}\"):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    accuracy_train.append(test_accuracy)\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(epoch + 1, n_epochs, loss.item(), test_accuracy * 100))\n",
    "    loss_train.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6db4a-bc2b-4fe6-8163-d04396dcbff2",
   "metadata": {},
   "source": [
    "Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "bbe22525-3885-4a88-b984-3a69c2c9fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, columns_to_drop=['hash_id', 'source_id']):\n",
    "    df = pd.read_json(path, lines=True).drop(columns=columns_to_drop)\n",
    "    df['audio_path'] = df['audio_path'].apply(lambda x: x.split('/')[1])\n",
    "    df = df[df['annotator_emo'] != 'other']\n",
    "    #df = df[df['duration'] <= 5.0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b4c1d7b1-a95b-4150-88dd-c8ca185197bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tar crowd.tar extracted\n"
     ]
    }
   ],
   "source": [
    "tar_file_path = 'crowd.tar'\n",
    "\n",
    "with tarfile.open(tar_file_path, 'r') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "print(f'Tar {tar_file_path} extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "564f2005-7b24-4970-b6f3-0b3d640af9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tar podcast.tar extracted\n"
     ]
    }
   ],
   "source": [
    "tar_file_path = 'podcast.tar'\n",
    "\n",
    "with tarfile.open(tar_file_path, 'r') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "print(f'Tar {tar_file_path} extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "344232ee-35c9-451a-be13-324a3dafe043",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_train = load_data('crowd_train/raw_crowd_train.jsonl')\n",
    "crowd_test = load_data('crowd_test/raw_crowd_test.jsonl')\n",
    "podcast_train = load_data('podcast_train/raw_podcast_train.jsonl')\n",
    "podcast_test = load_data('podcast_test/raw_podcast_test.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2f23c-22cb-4f75-b99b-bbf50214351e",
   "metadata": {},
   "source": [
    "### LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "27cd9500-cf9b-46cb-9059-af8ae7954394",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit_transform(['angry', 'sad', 'neutral', 'positive']) # class order\n",
    "le.classes_ = np.array(['angry', 'sad', 'neutral', 'positive'])\n",
    "crowd_train['annotator_emo'] = le.transform(crowd_train['annotator_emo'])\n",
    "crowd_test['annotator_emo'] = le.transform(crowd_test['annotator_emo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1b78ced3-8749-44eb-a018-ebc9e2323a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475e76f77ac1ed7cabafca740b15b32a.wav</td>\n",
       "      <td>2.453000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>не надо не надо не надо не надо</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f9438ef68395c70a8714dc373a49d11.wav</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>фозил кори mp три</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9937036a9c0dba20eecbffddd00f2be2.wav</td>\n",
       "      <td>4.341750</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb0ae78586a235018103acec22a80a8f.wav</td>\n",
       "      <td>3.900562</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>сколько стоит на керамбит</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196dcf9e1aaac46c2aee45e7f6adfb92.wav</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>афина когда закончится эта телепередача</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             audio_path  duration  annotator_emo  golden_emo  \\\n",
       "0  475e76f77ac1ed7cabafca740b15b32a.wav  2.453000              0         NaN   \n",
       "1  2f9438ef68395c70a8714dc373a49d11.wav  4.640000              2         NaN   \n",
       "2  9937036a9c0dba20eecbffddd00f2be2.wav  4.341750              2         2.0   \n",
       "3  fb0ae78586a235018103acec22a80a8f.wav  3.900562              2         NaN   \n",
       "4  196dcf9e1aaac46c2aee45e7f6adfb92.wav  4.780000              2         NaN   \n",
       "\n",
       "                       annotator_id                             speaker_text  \\\n",
       "0  858305a5450b7bd1288ba0053b1cd1c1          не надо не надо не надо не надо   \n",
       "1  858305a5450b7bd1288ba0053b1cd1c1                        фозил кори mp три   \n",
       "2  858305a5450b7bd1288ba0053b1cd1c1                                     None   \n",
       "3  858305a5450b7bd1288ba0053b1cd1c1                сколько стоит на керамбит   \n",
       "4  858305a5450b7bd1288ba0053b1cd1c1  афина когда закончится эта телепередача   \n",
       "\n",
       "  speaker_emo  \n",
       "0       angry  \n",
       "1     neutral  \n",
       "2        None  \n",
       "3     neutral  \n",
       "4     neutral  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e92724f9-b6cc-43e2-9d6c-a63aaae85930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9961c53ca6eeb440b217e539fbf46c.wav</td>\n",
       "      <td>5.82</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>я слушаю</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0166f65a30354db8282682b1a280e64c.wav</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d49a6b560155831725a7bdc7d0a96099.wav</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>где родился шерлок холмс</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c6852b0925797612d7b6724da8cbe7b4.wav</td>\n",
       "      <td>8.58</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>открой в браузере ennio morricone</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64a7aa17132c3e4b7be1aaed5fc88090.wav</td>\n",
       "      <td>5.06</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32bd471407fe168dacd5f8252f9949b7</td>\n",
       "      <td>а там и ева проснулись с утра</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              audio_path  duration  annotator_emo  golden_emo  \\\n",
       "0   9e9961c53ca6eeb440b217e539fbf46c.wav      5.82              2         NaN   \n",
       "1   0166f65a30354db8282682b1a280e64c.wav      3.70              1         NaN   \n",
       "2   d49a6b560155831725a7bdc7d0a96099.wav      4.38              2         NaN   \n",
       "3   c6852b0925797612d7b6724da8cbe7b4.wav      8.58              2         NaN   \n",
       "12  64a7aa17132c3e4b7be1aaed5fc88090.wav      5.06              3         NaN   \n",
       "\n",
       "                        annotator_id                       speaker_text  \\\n",
       "0   858305a5450b7bd1288ba0053b1cd1c1                           я слушаю   \n",
       "1   858305a5450b7bd1288ba0053b1cd1c1                    каким стал сбер   \n",
       "2   858305a5450b7bd1288ba0053b1cd1c1           где родился шерлок холмс   \n",
       "3   858305a5450b7bd1288ba0053b1cd1c1  открой в браузере ennio morricone   \n",
       "12  32bd471407fe168dacd5f8252f9949b7      а там и ева проснулись с утра   \n",
       "\n",
       "   speaker_emo  \n",
       "0      neutral  \n",
       "1      neutral  \n",
       "2      neutral  \n",
       "3      neutral  \n",
       "12    positive  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c38109-85da-4daa-8911-5b406e3a1c6a",
   "metadata": {},
   "source": [
    "## Подготовка данных для обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152717b-0a85-4959-87f4-d5d01f4e5686",
   "metadata": {},
   "source": [
    "Предобработка делаем через torchaudio так как она быстрее и работает на gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "6d5ef251-2f57-4e53-a12c-c6ca8a958958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DushaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, audio_dir, transformation,\n",
    "                target_sample_rate, num_samples, device):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "\n",
    "        signal, sr = torchaudio.load(self.audio_dir + audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        # signal (num_channels, samples) -> (2, 16000) -> (1, 16000)\n",
    "        signal = self._mix_down_if_necessary(signal) # if have diffrent chanels\n",
    "        signal = self._resample_if_necessary(signal,sr) # if diffrent freq\n",
    "        signal = self._cut_if_neccessary(signal)\n",
    "        signal = self._right_pad_if_neccessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal , label\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim = 0, keepdim = True)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_neccessary(self, signal):\n",
    "        # signal -> Tensor -> (1, num_samples)\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:,:self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_neccessary(self, signal):\n",
    "        lenght_signal = signal.shape[1]\n",
    "        if lenght_signal < self.num_samples:\n",
    "            num_miising_samples = self.num_samples = lenght_signal\n",
    "            last_dim_padding = (0, num_miising_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = self.df.iloc[index,0]\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        label = self.df.iloc[index,2]\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac24f51-def5-4faa-acd7-a73c2c831c12",
   "metadata": {},
   "source": [
    "Распределние фурье, мелспектограмма те частоты которые понимает человек, потому что фурье может быть на частотах которые человек не понимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6c34a341-995e-476d-bce4-ed20d61ae09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 80000 # 5 sec (5 * sample_rate)\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64)\n",
    "audio_dir_train = \"/Users/kirillanpilov/NLP_FU/Lab6/crowd_train/wavs/\"\n",
    "audio_dir_test = \"/Users/kirillanpilov/NLP_FU/Lab6/crowd_test/wavs/\"\n",
    "dusha_dataset_train = DushaDataset(crowd_train, audio_dir_train, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)\n",
    "dusha_dataset_test = DushaDataset(crowd_test, audio_dir_test, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c55d3796-fbfc-49e2-b6dc-885de8d2785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, label = dusha_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "66a7fae0-30b9-46e8-91ef-a1059e3a412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 154])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.shape # (num_channels, n_mels, time_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "553da105-cd88-4749-87a3-07133cb6125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705fa7b-8889-4f8b-872c-e470d83a9d16",
   "metadata": {},
   "source": [
    "## 2. Решить задачу классификации классическими методами машинного обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf4df2-f8e0-44a0-9cf2-62883fb0d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "#Train\n",
    "for i in range(len(dusha_dataset_train)):\n",
    "    signal, label = dusha_dataset_train[i]\n",
    "    X_train.append(signal.flatten().numpy())\n",
    "    y_train.append(label)\n",
    "\n",
    "#Test\n",
    "for i in range(len(dusha_dataset_test)):\n",
    "    signal, label = dusha_dataset_test[i]\n",
    "    X_test.append(signal.flatten().numpy())\n",
    "    y_test.append(label)\n",
    "\n",
    "print(\"Train\")\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test\")\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127e1d0",
   "metadata": {},
   "source": [
    "## Обучение моделей <a class=\"anchor\" id=\"fifth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4391332",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d519b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b188f3-0028-450e-8ea5-91f62483e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cnn.to(device), (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c433f-efb8-4f94-a966-135466cdc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5\n",
    "\n",
    "\n",
    "model = CNNNetwork().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1628b1-1c9f-4cff-957e-e7c040cd4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, criterion, optimizer, EPOCHS, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mx5XybireCna",
   "metadata": {
    "id": "Mx5XybireCna"
   },
   "source": [
    "Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae7d84d-4256-48c4-9f38-77775bbca879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1765b92b-db0c-4677-b1d6-ee4cb5d346be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d15577e2-6c70-4521-a4f0-55422c1dd808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a0766efea14c75afd780b69568453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.wav', description='Upload Audio File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def transcribe_uploaded_audio(change):\n",
    "    # Получение содержимого загруженного файла\n",
    "    file_contents = file_upload.data[-1]['content']\n",
    "    audio_data, sample_rate = torchaudio.load(io.BytesIO(file_contents), normalize=True)\n",
    "\n",
    "    # Воспроизведение аудио\n",
    "    display(Audio(audio_data.numpy(), rate=sample_rate))\n",
    "\n",
    "    # Распознавание речи\n",
    "    result = pipe(audio_data.squeeze().numpy())\n",
    "    transcription_text = result[\"text\"]\n",
    "\n",
    "    # Вывод результатов\n",
    "    print(\"Transcription result:\", transcription_text)\n",
    "\n",
    "# Создание виджета для загрузки файла\n",
    "file_upload = widgets.FileUpload(accept='.wav', description=\"Upload Audio File\")\n",
    "file_upload.observe(transcribe_uploaded_audio, names='data')\n",
    "\n",
    "# Отображение виджета\n",
    "display(file_upload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0-8W6fOkeJEv",
   "metadata": {
    "id": "0-8W6fOkeJEv"
   },
   "source": [
    "## 5. Примените ruBERT для анализа тональности текста. Сделать выводы по каждому из заданий!!! Примечание!!! Если не хватает вычислительных ресурсов для работы с датасетом Dusha, то можно использовать датасет, в котором объединены датасеты SAVEE и TESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e09d09",
   "metadata": {},
   "source": [
    "Предобученная модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d21606-abdc-4d2b-89e7-674ab461782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hubert():\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        \"facebook/hubert-large-ls960-ft\")\n",
    "    model = HubertForSequenceClassification.from_pretrained(\n",
    "        \"xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned\")\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e56842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hubert, processor_hubert = load_model_hubert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2emotion = {0: 'neutral', 1: 'angry', 2: 'positive', 3: 'sad', 4: 'other'}\n",
    "    inputs = processor_hubert(\n",
    "        audio,\n",
    "        sampling_rate=processor_hubert.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        max_length=16000 * 10,\n",
    "        truncation=True\n",
    "    )\n",
    "    logits = model_hubert(inputs['input_values'][0]).logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predicted_emotion = num2emotion[predictions.numpy()[0]]\n",
    "    return predicted_emotion"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
