{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0IvyV6_EdcFa",
   "metadata": {
    "id": "0IvyV6_EdcFa"
   },
   "source": [
    "## Cодержание:\n",
    "* [Импорт библиотек](#first)\n",
    "* [Загрузка и изучение данных](#second)\n",
    "* [Базовый анализ данных](#third)\n",
    "* [Предобработка данных](#fourth)\n",
    "* [Обучение модели](#fifth)\n",
    "* [Тестирование модели](#sixth)\n",
    "* [Выводы](#seventh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m4Hw1xcOeRlB",
   "metadata": {
    "id": "m4Hw1xcOeRlB"
   },
   "source": [
    "## Импорт библиотек <a class=\"anchor\" id=\"first\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8880ce28-f137-465a-aad3-0dcd008a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install noisereduce\n",
    "# pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "OMe8JqWHeavp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "OMe8JqWHeavp",
    "outputId": "05181c2e-7d19-4bea-9335-521739e84024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from IPython.display import Audio\n",
    "import noisereduce as nr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfba834-685d-4547-bbc1-3afd72cd7b53",
   "metadata": {},
   "source": [
    "## Функция обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0451fe-51d4-4ace-b844-5de00d4915c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, n_epochs, train_loader, test_loader):\n",
    "\n",
    "  loss_train = []\n",
    "  accuracy_train = []\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for input, target in tqdm(train_loader, desc=f\"Training epoch {epoch + 1}/{n_epochs}\"):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for input, target in tqdm(test_loader, desc=f\"Testing epoch {epoch + 1}/{n_epochs}\"):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    accuracy_train.append(test_accuracy)\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(epoch + 1, n_epochs, loss.item(), test_accuracy * 100))\n",
    "    loss_train.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8RWijZA6dPV4",
   "metadata": {
    "id": "8RWijZA6dPV4"
   },
   "source": [
    "## Загрузка и изучение данных <a class=\"anchor\" id=\"second\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a447c023-2374-4e7d-a11c-bc37a7431ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://n-ws-q0bez.s3pd12.sbercloud.ru/b-ws-q0bez-jpv/dusha/crowd.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6db4a-bc2b-4fe6-8163-d04396dcbff2",
   "metadata": {},
   "source": [
    "Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "bbe22525-3885-4a88-b984-3a69c2c9fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, columns_to_drop=['hash_id', 'source_id']):\n",
    "    df = pd.read_json(path, lines=True).drop(columns=columns_to_drop)\n",
    "    df['audio_path'] = df['audio_path'].apply(lambda x: x.split('/')[1])\n",
    "    df = df[df['annotator_emo'] != 'other']\n",
    "    #df = df[df['duration'] <= 5.0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b4c1d7b1-a95b-4150-88dd-c8ca185197bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tar crowd.tar extracted\n"
     ]
    }
   ],
   "source": [
    "tar_file_path = 'crowd.tar'\n",
    "\n",
    "with tarfile.open(tar_file_path, 'r') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "print(f'Tar {tar_file_path} extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "564f2005-7b24-4970-b6f3-0b3d640af9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tar podcast.tar extracted\n"
     ]
    }
   ],
   "source": [
    "tar_file_path = 'podcast.tar'\n",
    "\n",
    "with tarfile.open(tar_file_path, 'r') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "print(f'Tar {tar_file_path} extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "344232ee-35c9-451a-be13-324a3dafe043",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_train = load_data('crowd_train/raw_crowd_train.jsonl')\n",
    "crowd_test = load_data('crowd_test/raw_crowd_test.jsonl')\n",
    "podcast_train = load_data('podcast_train/raw_podcast_train.jsonl')\n",
    "podcast_test = load_data('podcast_test/raw_podcast_test.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c38109-85da-4daa-8911-5b406e3a1c6a",
   "metadata": {},
   "source": [
    "## Подготовка данных для обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152717b-0a85-4959-87f4-d5d01f4e5686",
   "metadata": {},
   "source": [
    "Предобработка делаем через torchaudio так как она быстрее и работает на gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "6d5ef251-2f57-4e53-a12c-c6ca8a958958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DushaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, audio_dir, transformation,\n",
    "                target_sample_rate, num_samples, device):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "\n",
    "        signal, sr = torchaudio.load(self.audio_dir + audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        # signal (num_channels, samples) -> (2, 16000) -> (1, 16000)\n",
    "        signal = self._mix_down_if_necessary(signal) # if have diffrent chanels\n",
    "        signal = self._resample_if_necessary(signal,sr) # if diffrent freq\n",
    "        signal = self._cut_if_neccessary(signal)\n",
    "        signal = self._right_pad_if_neccessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal , label\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim = 0, keepdim = True)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_neccessary(self, signal):\n",
    "        # signal -> Tensor -> (1, num_samples)\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:,:self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_neccessary(self, signal):\n",
    "        lenght_signal = signal.shape[1]\n",
    "        if lenght_signal < self.num_samples:\n",
    "            num_miising_samples = self.num_samples = lenght_signal\n",
    "            last_dim_padding = (0, num_miising_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = self.df.iloc[index,0]\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        label = self.df.iloc[index,2]\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac24f51-def5-4faa-acd7-a73c2c831c12",
   "metadata": {},
   "source": [
    "Распределние фурье, мелспектограмма те частоты которые понимает человек, потому что фурье может быть на частотах которые человек не понимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6c34a341-995e-476d-bce4-ed20d61ae09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 80000 # 5 sec (5 * sample_rate)\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64)\n",
    "audio_dir_train = \"/Users/kirillanpilov/NLP_FU/Lab6/crowd_train/wavs/\"\n",
    "audio_dir_test = \"/Users/kirillanpilov/NLP_FU/Lab6/crowd_test/wavs/\"\n",
    "dusha_dataset_train = DushaDataset(crowd_train, audio_dir_train, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)\n",
    "dusha_dataset_test = DushaDataset(crowd_test, audio_dir_test, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c55d3796-fbfc-49e2-b6dc-885de8d2785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, label = dusha_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "66a7fae0-30b9-46e8-91ef-a1059e3a412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 154])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.shape # (num_channels, n_mels, time_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "553da105-cd88-4749-87a3-07133cb6125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mx5XybireCna",
   "metadata": {
    "id": "Mx5XybireCna"
   },
   "source": [
    "Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae7d84d-4256-48c4-9f38-77775bbca879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1765b92b-db0c-4677-b1d6-ee4cb5d346be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d15577e2-6c70-4521-a4f0-55422c1dd808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a0766efea14c75afd780b69568453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.wav', description='Upload Audio File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def transcribe_uploaded_audio(change):\n",
    "    # Получение содержимого загруженного файла\n",
    "    file_contents = file_upload.data[-1]['content']\n",
    "    audio_data, sample_rate = torchaudio.load(io.BytesIO(file_contents), normalize=True)\n",
    "\n",
    "    # Воспроизведение аудио\n",
    "    display(Audio(audio_data.numpy(), rate=sample_rate))\n",
    "\n",
    "    # Распознавание речи\n",
    "    result = pipe(audio_data.squeeze().numpy())\n",
    "    transcription_text = result[\"text\"]\n",
    "\n",
    "    # Вывод результатов\n",
    "    print(\"Transcription result:\", transcription_text)\n",
    "\n",
    "# Создание виджета для загрузки файла\n",
    "file_upload = widgets.FileUpload(accept='.wav', description=\"Upload Audio File\")\n",
    "file_upload.observe(transcribe_uploaded_audio, names='data')\n",
    "\n",
    "# Отображение виджета\n",
    "display(file_upload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0-8W6fOkeJEv",
   "metadata": {
    "id": "0-8W6fOkeJEv"
   },
   "source": [
    "## 5. Примените ruBERT для анализа тональности текста. Если не хватает вычислительных ресурсов для работы с датасетом Dusha, то можно использовать датасет, в котором объединены датасеты SAVEE и TESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e09d09",
   "metadata": {},
   "source": [
    "Предобученная модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d21606-abdc-4d2b-89e7-674ab461782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hubert():\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        \"facebook/hubert-large-ls960-ft\")\n",
    "    model = HubertForSequenceClassification.from_pretrained(\n",
    "        \"xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned\")\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e56842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hubert, processor_hubert = load_model_hubert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2emotion = {0: 'neutral', 1: 'angry', 2: 'positive', 3: 'sad', 4: 'other'}\n",
    "    inputs = processor_hubert(\n",
    "        audio,\n",
    "        sampling_rate=processor_hubert.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        max_length=16000 * 10,\n",
    "        truncation=True\n",
    "    )\n",
    "    logits = model_hubert(inputs['input_values'][0]).logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predicted_emotion = num2emotion[predictions.numpy()[0]]\n",
    "    return predicted_emotion"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
